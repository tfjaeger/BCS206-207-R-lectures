---
title: "PhD Skillz: Sampling-based approaches"
author: T. Florian Jaeger
email: fjaeger@ur.rochester.edu
format:
  revealjs:
    transition: slide
    background-transition: fade
    theme: [night, custom.scss]
editor: visual
editor_optons:
  canonical: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  results = "markup", warning = FALSE, cache = TRUE,
  fig.align = "center", fig.width = 6.5)
```

```{r constants, include = F}
n.samples = 2000
```

```{r libraries, include = F}
library("tidyverse")  # dplyr, ggplot2, and more from the tidyverse
library("magrittr")   # for pipes
library("boot")       # for automatic bootstrap
```

## Imagine

::: incremental
-   You have 1 coin. You want to know whether it's biased.

    -   You flip the coin 100,000 times, and each of the two sides occurred 50,000 times. Is the coin biased?
    -   You flip the coin 100,000 times, and side A occurred 99,000 times, side B occurred 1,000 times each. Is the coin biased?

-   Easy, right?
:::

## Imagine *a little harder* {auto-animate="true"}

::: incremental
-   You flip the coin 40 times, with the following outcome: side A occurred 26 times, side B occurred 14 times. Is the coin biased?

-   How might you determine the answer?

    -   Analytically. For this scenario, there is a known analytical solution, and it's available, e.g., through a $\chi^2$ test.
:::

## Imagine *a little harder* {auto-animate="true"}

-   You roll the die 40 times, with the following outcome: side A occurred 26 times, side B occurred 14 times. Is the coin biased?

-   How might you determine the answer?

    -   Analytically. For this scenario, there is a known analytical solution, and it's available, e.g., through an exact binomial test:

```{r, echo=TRUE}
binom.test(x = c(26, 14), p = c(.5), alternative = "two.sided")
```

## Imagine *a little harder* {auto-animate="true"}

-   You roll the die 40 times, with the following outcome: side A occurred 26 times, side B occurred 14 times. Is the coin biased?

-   How might you determine the answer?

    -   Analytically. For this scenario, there is a known analytical solution, and it's available, e.g., through a $\chi^2$ test.

    -   Through simulation.

## A simulation-based approach: How would you proceed? {auto-animate="true"}

-   **The null.** An unbiased coin should have equal probabilities to land on either side (.5/.5)

## A simulation-based approach: How would you proceed? {auto-animate="true"}

-   The null
-   **Data generation process for the null.** Resample 40 draws from a fair coin flip.

```{r}
generate_data_under_null <- function() rbinom(n = 40, size = 1, prob  = .5)
generate_data_under_null()
```

## A simulation-based approach: How would you proceed? {auto-animate="true"}

-   The null
-   Data generation process for the null
-   **Summarize data down to the key statistic(s):** Here, a sufficient statistic is how often the more frequent outcome occurs.

```{r, echo=TRUE}
summarize_data <- function(x) x %>% table() %>% max()

generate_data_under_null() %>%
  summarize_data()
```

## A simulation-based approach: How would you proceed? {auto-animate="true"}

-   The null
-   Data generation process for the null
-   Summarize data down to the key statistic(s)
-   **Repeat many times while storing statistic(s):** This forms our baseline/null distribution

```{r, echo=TRUE}
null_distribution <- replicate(
  n = 20000,
  expr = {generate_data_under_null() %>% summarize_data()})
head(null_distribution, 20)
```

## A simulation-based approach: How would you proceed? {auto-animate="true"}

-   The null
-   Data generation process for the null
-   Summarize data down to the key statistic(s)
-   Repeat many times while storing statistic(s)
-   **Compare how often simulation yielded values at least as extreme as actual observation:** This is our p-value.

```{r, echo=TRUE}
sum(null_distribution >= 26) / sum(!is.na(null_distribution))
```

## Using simulations to estimate the *Type I error rate* of an analysis

1.  We generate data under some assumption (e.g., a Normally distributed outcome) or resample it from existing real data.
2.  We add zero effect.
3.  We apply whatever type(s) of analysis we're interested in estimating Type I error for
4.  We repeat this process many times, each time storing whether the p-value(s) of interest were less than our significance criterion.
5.  The proportion of significant results is the Type I error rate.

## An example problem

```{r, include = F}
# We will generate some fake data to work with, but you could plug your own real data in here
# (and change the code below so that it matches the names of the variables in your data)
set.seed(123456)

n.subject = 48
n.trial = 24
```

-   Experiment with **`r n.subject` subjects**, in which each subject saw **`r n.trial` trials** for each of two conditions (for a total of `r n.trial * 2` trials per subject).

-   On each trial, subjects saw a picture of an object and had to decide as quickly and accurately whether the picture depicts something animate (a living thing) or not.

-   IVs: **Two conditions** manipulate whether the object is something that we see frequently in our life ("high frequency") or not ("low frequency").

-   DVs: **Reaction times** of subjects' responses (`RT`), and **accuracy** (`correct`).

-   H$_1$: more frequent objects will be responded to more quickly and accurately.

```{r}
# Let's make some (fake) data
d.orig <-
  crossing(
    subject = 1:n.subject,
    condition = c("low frequency","high frequency"),
    trial = 1:n.trial
  ) %>%
  mutate(
    muLogOddsCorrect = qlogis(case_when(
      condition == "low frequency" ~ .78,
      condition == "high frequency" ~ .91
    )),
    muLogRT = case_when(
      condition == "low frequency" ~ 6.5,
      condition == "high frequency" ~ 6.1
    )
  ) %>%
  group_by(subject, condition) %>%
  mutate(
    muLogOddsCorrect.bySubject = rnorm(1, 0, .75),
    muLogRT.bySubject = rnorm(1, 0, .4)
  ) %>%
  rowwise() %>%
  mutate(
    correct = rbinom(1, 1, plogis(muLogOddsCorrect + muLogOddsCorrect.bySubject)),
    RT = round(100 + exp(rnorm(1, muLogRT + muLogRT.bySubject - .15 * correct, .1)), 3)
  ) %>%
  as_tibble() %>%
  mutate_at(c("condition", "subject"), factor) %>%
  arrange(subject, trial, condition)

d <- d.orig %>%
    select(-starts_with("mu"))
```

## Parametrically estimating Type I error

```{r, include=FALSE}
# Which subject is chosen for simulation?
which.subj = 3
```

-   We would like to assess the Type I error rate of subject-wise $t$-tests for both the RT and the accuracy data.

-   For this simulation, we select Subject `r which.subj`.

## 1: Data generation {auto-animate="true"}

-   Get the mean and SD for both RTs and accuracy

```{r, echo=TRUE}
#| code-fold: true
#| code-summary: "expand for full code"
#| fig-align: "center"
d.pars <-
  d %>%
  filter(subject == which.subj) %>%
  summarise(
    across(
      c(RT, correct),
      list("mean" = mean, "sd" = sd),
      .names = "{.fn}_{.col}"))

d.pars
```

## 1: Data generation {auto-animate="true"}

-   Get the mean and SD for both RTs and accuracy
-   Function to generate RT and accuracy data from these parameters.

```{r, echo = T}
#| code-fold: true
#| code-summary: "expand for full code"
#| fig-align: "center"
make_single_subject_data <- function(n.trial, d.pars) {
  # This generates a data frame with one row for each unique
  # combination of the variable values handed to it. E.g., if
  # there are 24 trials (n.trial = 24), we will end up with
  # 48 rows (= 1 subject * 2 conditions * 48 trials)
  crossing(
    subject = 1,
    condition = c("low frequency","high frequency"),
    trial = 1:n.trial
  ) %>%
    # Sample the responses / outcomes / dependent variables, i.e.
    # the accuracy and RTs. d.pars is a data frame with the mean
    # and SD of the RT and accuracy distributions we want to sample.
    mutate(
      correct = rbinom(nrow(.), 1, d.pars$mean_correct[1]),
      RT = rnorm(nrow(.), d.pars$mean_RT[1], d.pars$sd_RT[1])) %>%
    # Do some formatting
    as_tibble() %>%
    mutate_at(c("condition", "subject"), factor) %>%
    arrange(subject, trial, condition)
}
```

## 1: Data generation {auto-animate="true"}

-   Get the mean and SD for both RTs and accuracy
-   Function to generate RT and accuracy data from these parameters.
-   Example draw from data generation function:

```{r}
temp <- make_single_subject_data(n.trial, d.pars)
print(temp)
```

. . .

-   Condition means and SEs:

```{r}
temp %>%
  group_by(condition) %>%
  summarise(
    across(
      c(RT, correct),
      list(
        "mean" = ~ mean(.x, na.rm = T),
        "SE" = ~ sd(.x, na.rm = T) / sqrt(sum(!is.na(.x)) - 1)),
      .names = "{.fn}_{.col}"))
```

::: notes
-   Note that the RT and accuracy values for the two conditions are not identical. That's to be expected since each trial is generated as a draw from a random variable.
-   The critical question is how this affects our conclusions about significant differences between the means of the conditions.
:::

## 2: Analysis

-   Function that applies the two $t$-tests to the data, and stores the $p$-value.

```{r, echo=TRUE}
#| code-fold: true
#| code-summary: "expand for full code"
#| fig-align: "center"
do_test <- function(data) {
  d = tibble(.rows = 1)

  if ("RT" %in% names(data)) {
    t1 = t.test(RT ~ condition, data = data, var.equal = T)
    d %<>%
      mutate(RT.p.value = t1$p.value)
  }
  if ("correct" %in% names(data)) {
    t2 = t.test(correct ~ condition, data = data, var.equal = T)
    d %<>%
      mutate(Accuracy.p.value = t2$p.value)
  }

  return(d)
}
```

. . .

-   Example application to the data generated on the previous slide:

```{r}
temp %>%
  do_test()
```

## 3: Repeat many times {auto-animate="true"}

-   Repeat steps 1 and 2, 1000 times.

```{r, echo=TRUE, warning=FALSE}
#| code-fold: true
#| code-summary: "expand for full code"
#| fig-align: "center"
# rdply does more less the same as replicate() but conveniently binds
# the results into a tibble, with one row for each of the .n outputs
# that result from running each of the .n repetitions of the r code.
d.type1 =
  plyr::rdply(
    # How many simulations/samples do we want to generate and analyze?
    .n = n.samples,
    # What we want to be repeated:
    function(i) {
      dd <- try(
        make_single_subject_data(n.trial, d.pars)  %>%
          do_test(),
        silent = TRUE)
      # t.test sometimes fail. I'm catching those cases
      # and setting both p.values to NA
      if (any(class(dd) == "try-error"))
        dd = data.frame(
          RT.p.value = NA,
          Accuracy.p.value = NA
        )
      return(dd)
    })
```

## 3: Repeat many times {auto-animate="true"}

-   Repeat steps I and II, 1000 times.

-   Here are the first 20 rows of p-values resulting from that simulation:

```{r}
d.type1 %>%
  head(20)
```

::: notes
t-test sometimes does not converge. Hence the NAs.
:::

## 4: Calculate Type I error rate {auto-animate="true"}

-   Calculate proportion of the 1000 samples for which conditions had significant effect:

```{r, echo=TRUE}
#| code-fold: true
#| code-summary: "expand for full code"
#| fig-align: "center"
get_TypeI <-
  . %>%
  summarise(
    across(
      ends_with("p.value"),
      list(
        "convergence" = ~ sum(!is.na(.x)) / length(.x),
        "Type_I_error" =  
          function(x) {
            # Here we count t-tests that yield an NA results as *not* a Type I error.
            # This assumes an experimenter who got such a result (hopefully!) would not
            # conclude that they found a significant effect.
            x <- ifelse(is.na(x), 1, x)
            round(mean(x < .05), 3)
      }),
    .names = "{.fn}_{.col}")) %>%
  rename_all(.funs = ~ gsub("\\.p\\.value", "", .x)) %>%
  # Since we're setting analyses to NA when either of the RT
  # or accuracy analysis failed, we only need one convergence rate
  select(-convergence_Accuracy) %>%
  rename(convergence = convergence_RT)
```

```{r}
d.type1 %>%
  get_TypeI()
```

. . .

-   Type I error rates look OKish for RTs, but clearly conservative for accuracy.

## 4: Calculate Type I error rate {auto-animate="true"}

-   Calculate proportion of the 1000 samples for which conditions had significant effect:

-   Type I error rates look OKish for RTs, but clearly conservative for accuracy.

-   **Type I error calculation only as good as its assumptions.** Here, we've made some problematic assumptions.

::: notes
For example, we assumed that RTs are drawn from a normal distribution, but that would predict that there can be negative RTs (the tails of a normal distribution are infinitely long).
:::

## Consequences of problematic assumptions about ground truth

-   Problematic assumptions do not *necessarily* lead to wrong estimates of the Type I error rate, but they *can*.

-   Here, we actually *know* the ground truth. That's because the data analyzed here are actually not data from a psycholinguistic experiment.

-   Instead, they are generated at the top of this document, using:

    -   log-normal (rather than normal) distribution for the RTs
    -   Bernoulli distribution for accuracies

. . .

-   We can compare the Type I error rate of the $t$-test obtained above to the Type I error rate of the same $t$-test if the data is generated following the ground truth (see [Git repo](https://github.com/tfjaeger/BCS206-207-R-lectures/blob/2024-PhD-skillz-workshop/scripts/R/6-Type-I-error-and-power-analyses.Rmd)).

## What if we don't know the ground truth?

-   IRL, we rarely know the ground truth. How then, can we avoid problematic assumptions?
-   No magic bullet, but **non-parametric bootstrap** avoids specific parametric assumptions.

. . .

-   Specifically, we can substitute steps 1 and 2 by sampling without replacement data points from subject `r which.subj`:

```{r, echo=TRUE}
#| code-fold: true
#| code-summary: "expand for full code"
#| fig-align: "center"
bootstrap_single_subject_data = function(n.trial, d) {
  d %>%
    filter(subject == which.subj) %>%
    ungroup() %>%
    # Sample twice as many rows as n.trials since there are two conditions
    sample_n(n.trial * 2, replace = T) %>%
    # Randomly assign data to conditions (since we are simulating a null effect)
    mutate(condition = rep(c("high frequency", "low frequency"), n.trial))
}
```

## What if we don't know the ground truth?

-   Applying the same steps 3 and 4 as above, we find:

```{r}
d.type1.boot <-
  plyr::rdply(
    .n = n.samples,
    function(i) {
      dd = try(bootstrap_single_subject_data(n.trial, d) %>%
                 do_test(), silent = TRUE)
      if (any(class(dd) == "try-error"))
        dd = data.frame(
          RT.p.value = NA,
          Accuracy.p.value = NA
        )
      return(dd)
    })

d.type1.boot %>%
  get_TypeI()
```

-   Improving the Type I error rate for RTs by bringing it closer to .05.
-   Type I error rate for accuracy remains conservative. $t$-tests are not suited for such data.

## Using simulations to estimate *statistical power* of an analysis

Can you guess what we'd need to add to turn this approach into a power analysis?

1.  [We generate data under some assumption (e.g., a Normally distributed outcome) or resample it from existing real data.]{style="color:gray"}
2.  [We add a zero effect.]{style="color:gray"}
3.  [We apply whatever type(s) of analysis we're interested in estimating **Type I error rate** for]{style="color: gray"}
4.  [We repeat this process many times, each time storing whether the p-value(s) of interest were less than our significance criterion.]{style="color: gray"}
5.  [The proportion of significant results is the **Type I error rate**.]{style="color:gray"}

## Using simulations to estimate *statistical power* of an analysis

Can you guess what we'd need to add to turn this approach into a power analysis?

1.  [We generate data under some assumption (e.g., a Normally distributed outcome) or resample it from existing real data.]{style="color:gray"}
2.  We add a **non-zero** effect that we want to estimate power for (e.g., by subtracting half of it from one condition and adding half of it to the other condition).
3.  [We apply whatever type(s) of analysis we're interested in estimating ]{style="color:gray"} **power** [for]{style="color: gray"}
4.  [We repeat this process many times, each time storing whether the p-value(s) of interest were less than our significance criterion.]{style="color: gray"}
5.  [The proportion of significant results is the ]{style="color:gray"} **power**.
